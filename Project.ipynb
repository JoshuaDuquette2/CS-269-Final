{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from timeit import default_timer as timer"
      ],
      "metadata": {
        "id": "h1-wBcY-cstM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html\n",
        "https://arxiv.org/pdf/2010.02663.pdf\n",
        "https://github.com/ucla-rlcourse/DeepRL-Tutorials/blob/master/12.A2C.ipynb"
      ],
      "metadata": {
        "id": "G3HADymPET76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurations"
      ],
      "metadata": {
        "id": "fBNNJRkPg8lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COLLISION_PENALTY  = -10\n",
        "COMPLETION_REWARD  = 100\n",
        "VISITATION_PENALTY = -10\n",
        "NUM_STEPS = 100\n",
        "ENCODER_OUTPUT_DIM = 128\n",
        "ROLLOUT = 100\n",
        "GAMMA = 0.99\n",
        "LR = 7e-4\n",
        "ENTROPY_LOSS_WEIGHT = 0.01\n",
        "VALUE_LOSS_WEIGHT = 0.5\n",
        "GRAD_NORM_MAX = 0.5\n",
        "NUM_ACTIONS = 8\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #cuda is gpu\n",
        "NUM_AGENTS = 3\n",
        "ENV_STEPS = 10000\n",
        "\n",
        "seed = 1\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "torch.set_num_threads(1)\n",
        "np.set_printoptions(linewidth=200)\n",
        "move_dict = {0:(1,0), 1:(1,1), 2:(0,1), 3:(-1,1), 4:(-1,0), 5:(-1,-1), 6:(0,-1), 7:(1,-1)}"
      ],
      "metadata": {
        "id": "7i8NcOThagc9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rollout Storage"
      ],
      "metadata": {
        "id": "zakNgHHchHnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RolloutStorage(object):\n",
        "    def __init__(self, num_steps, num_processes, obs_shape, state_shape, action_space, device):\n",
        "        self.observations = torch.zeros(num_steps + 1, num_processes, *obs_shape).to(device) # Repurpose number of processes to be number of environments\n",
        "        self.state = torch.zeros(num_steps + 1, *state_shape).to(device)\n",
        "        self.rewards = torch.zeros(num_steps, num_processes, 1).to(device)\n",
        "        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1).to(device)\n",
        "        self.returns = torch.zeros(num_steps + 1, num_processes, 1).to(device)\n",
        "        self.action_log_probs = torch.zeros(num_steps, num_processes, 1).to(device)\n",
        "        self.actions = torch.zeros(num_steps, num_processes, 1).to(device, torch.long)\n",
        "        self.masks = torch.ones(num_steps + 1, num_processes, 1).to(device)\n",
        "\n",
        "        self.num_steps = num_steps\n",
        "        self.step = 0\n",
        "\n",
        "    def insert(self, current_obs, action, action_log_prob, value_pred, reward, mask, state):\n",
        "        \"\"\"Insert values into rollout storage\"\"\"\n",
        "        self.observations[self.step + 1].copy_(current_obs)\n",
        "        self.actions[self.step].copy_(action)\n",
        "        self.action_log_probs[self.step].copy_(action_log_prob)\n",
        "        self.value_preds[self.step].copy_(value_pred)\n",
        "        self.rewards[self.step].copy_(reward)\n",
        "        self.masks[self.step + 1].copy_(mask)\n",
        "        self.state[self.step].copy_(state)\n",
        "\n",
        "        self.step = (self.step + 1) % self.num_steps\n",
        "\n",
        "    def after_update(self):\n",
        "        self.observations[0].copy_(self.observations[-1])\n",
        "        self.masks[0].copy_(self.masks[-1])\n",
        "\n",
        "    def compute_returns(self, next_value, gamma):\n",
        "        self.returns[-1] = next_value\n",
        "        for step in reversed(range(self.rewards.size(0))):\n",
        "            self.returns[step] = self.returns[step + 1] * \\\n",
        "                gamma * self.masks[step + 1] + self.rewards[step]"
      ],
      "metadata": {
        "id": "c1OGxU4JI-b4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Networks"
      ],
      "metadata": {
        "id": "Rhvt3DVjhOEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Embedding Network: Each agent encodes observations using a fully connected layer to \n",
        "enable heterogeneous teams\n",
        "input_dim = sensor package x sensor_package (e.g. 6*6=36 oberservations)\n",
        "output_dim > input_dim with a fixed feature length across all actors\n",
        "\n",
        "Input: Observation of size sensor package\n",
        "Output: Fixed length feature vector\n",
        "'''\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Linear(input_dim, output_dim)\n",
        "  def forward(self, obs):\n",
        "    if len(obs.size()) == 3:\n",
        "      obs = torch.flatten(obs, start_dim=1)\n",
        "    else:\n",
        "      obs = torch.flatten(obs)\n",
        "    \n",
        "    x = self.embedding(obs)\n",
        "    return x \n",
        "\n",
        "\n",
        "'''\n",
        "Critic Network: is a shared centralized critic to estimate value fuction V(St,ø) found by minimizing MSE loss\n",
        "Critic parameters ø are updated by mini-batch gradient descent\n",
        "At execution time, the critic is removed and agents excute their policies individually\n",
        "\n",
        "Input: State Information\n",
        "Output: Value estimate of the state\n",
        "'''\n",
        "class CriticNetwork(nn.Module):\n",
        "  def __init__(self, state_size, hidden_units=100):\n",
        "    super(CriticNetwork, self).__init__()\n",
        "    state_size = np.prod(state_size)\n",
        "\n",
        "    self.critic = nn.Sequential(\n",
        "        nn.Linear(state_size, hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_units, hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_units, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, state):\n",
        "    if len(state.size()) == 3:\n",
        "      state = torch.flatten(state, start_dim=1)\n",
        "    else:\n",
        "      state = torch.flatten(state)\n",
        "\n",
        "    value = self.critic(state) \n",
        "    return value\n",
        "\n",
        "'''\n",
        "Critic Network\n",
        "\n",
        "Input: State Information Unflattened\n",
        "Output: Value estimate of the state\n",
        "'''\n",
        "class CNNCritic(nn.Module):\n",
        "  def __init__(self, state_size, hidden_units=100):\n",
        "    super(CriticNetwork, self).__init__()\n",
        "\n",
        "    self.critic = nn.Sequential(\n",
        "        nn.Conv2d(state_size, hidden_units, kernel_size=3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(hidden_units, hidden_units*2, kernel_size=3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(hidden_units*2, hidden_units//2),\n",
        "        nn.Linear(hidden_units//2, 1) \n",
        "    )\n",
        "\n",
        "  def forward(self, state):\n",
        "    if len(state.size()) == 3:\n",
        "      state = torch.flatten(state, start_dim=0)\n",
        "    else:\n",
        "      state = state\n",
        "\n",
        "    value = self.critic(state) \n",
        "    return value\n",
        "  \n",
        "'''\n",
        "Actor Network: represents the agent policy function\n",
        "\n",
        "Input: fixed-length encoded observation so it's shareable among heterogenous agents\n",
        "Output: Action Logit\n",
        "'''\n",
        "class ActorNetwork(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, num_actions, hidden_units=100):\n",
        "    super(ActorNetwork, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(input_dim, output_dim)\n",
        "    self.actor = nn.Sequential(\n",
        "        nn.Linear(output_dim, hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_units, hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_units, num_actions)\n",
        "    )\n",
        "    self.train()\n",
        "\n",
        "  def forward(self, obs):\n",
        "    x = self.encoder(obs)\n",
        "    logits = self.actor(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "fSOEfyLYYbyT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExplorerAgent(object):\n",
        "  def __init__(self, loc, sensor_package=3):\n",
        "    self.x = loc[0]\n",
        "    self.y = loc[1]\n",
        "    self.sensor_package = sensor_package\n",
        "\n",
        "  def declare_networks(self):\n",
        "    self.model = ActorNetwork((self.sensor_package*2)**2, ENCODER_OUTPUT_DIM, NUM_ACTIONS)\n",
        "    self.optimizer = optim.RMSprop(self.model.parameters(), lr=LR, alpha=0.99, eps=1e-5)\n",
        "    self.model = self.model.to(DEVICE)\n",
        "\n",
        "  def move(self, action):\n",
        "    move = move_dict[action]\n",
        "    self.x += move[0]\n",
        "    self.y += move[1]\n",
        "\n",
        "  def set_location(self, loc):\n",
        "    self.x = loc[0]\n",
        "    self.y = loc[1]\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"Agent at x=\" + str(self.x) + \" and y=\" + str(self.y) + \" with sensor package=\" + str(self.sensor_package)"
      ],
      "metadata": {
        "id": "eLbXW95lLX73"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up Environment & Reward\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M_Af-p7z_u7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnknownTerrain(object):\n",
        "  '''\n",
        "  0 = passable\n",
        "  1 = impassible or out of bounds\n",
        "  2 = other agent\n",
        "  '''\n",
        "  def __init__(self, size=16, density=0.1, num_agents=3, padding=3):\n",
        "    self.size = size\n",
        "    self.padding = padding #if agent is at the edge of grid the environment will pad out impassable terrian so \n",
        "    # network has something to read\n",
        "    self.density = density #how much impassable terrian there is \n",
        "    self.observation_space = (padding * 2, padding * 2)\n",
        "    self.state_space = (size, size)\n",
        "    self.action_space = 8 # 8 cardinal directions\n",
        "    self.grid = np.zeros((size, size)).astype(np.uint8)\n",
        "    block_x = np.random.randint(0, size, size=int((size**2) * density))\n",
        "    block_y = np.random.randint(0, size, size=int((size**2) * density))\n",
        "    block_p = np.array(list(zip(block_x, block_y))) #generate random locations for obstacles\n",
        "    for x, y in block_p:\n",
        "      self.grid[x, y] = 1\n",
        "    self.num_agents = num_agents\n",
        "    self.locations = np.dstack(np.where(self.grid != 1)).squeeze() #generates viable agent placement locations\n",
        "    self.agents = [ExplorerAgent(self.locations[np.random.choice(self.locations.shape[0])]) for i in range(0, num_agents)]\n",
        "\n",
        "    self.grid = np.pad(self.grid, pad_width=padding, mode=\"constant\", constant_values=1) # pad out grid\n",
        "    #padding out grid\n",
        "    for agent in self.agents:\n",
        "      agent.x += padding # shift agents over with padding\n",
        "      agent.y += padding\n",
        "      self.grid[agent.x, agent.y] = 2 # assign grid location to agent\n",
        "\n",
        "\n",
        "    self.uncovered_grid = np.zeros((size, size)).astype(np.uint8)\n",
        "    self.rewards = {agent:0 for agent in self.agents} # Dictionary containing agents and their last step information\n",
        "    self.step_start_uncovered_ratio = 0.0 \n",
        "    self.last_move_uncovered_ratio = 0.0\n",
        "    self.step = 0\n",
        "    self.max_steps = NUM_STEPS\n",
        "\n",
        "\n",
        "  '''\n",
        "  Reset all changing information\n",
        "  '''\n",
        "  def reset(self):\n",
        "    self.grid = np.zeros((self.size, self.size)).astype(np.uint8)\n",
        "    block_x = np.random.randint(0, self.size, size=int((self.size**2) * self.density))\n",
        "    block_y = np.random.randint(0, self.size, size=int((self.size**2) * self.density))\n",
        "    block_p = np.array(list(zip(block_x, block_y)))\n",
        "    for x, y in block_p:\n",
        "      self.grid[x, y] = 1\n",
        "    self.locations = np.dstack(np.where(self.grid != 1)).squeeze()\n",
        "    self.grid = np.pad(self.grid, pad_width=self.padding, mode=\"constant\", constant_values=1) # need to adjust agent positions\n",
        "    for agent in self.agents:\n",
        "      agent.set_location(self.locations[np.random.choice(self.locations.shape[0])]) # Randomize location again\n",
        "      agent.x += self.padding \n",
        "      agent.y += self.padding\n",
        "      self.grid[agent.x, agent.y] = 2\n",
        "\n",
        "    self.uncovered_grid = np.zeros((self.size, self.size)).astype(np.uint8)\n",
        "    self.rewards = {agent:0 for agent in self.agents}\n",
        "    self.step_start_uncovered_ratio = 0.0 \n",
        "    self.last_move_uncovered_ratio = 0.0\n",
        "    self.step = 0\n",
        "\n",
        "    return self.observe_()\n",
        "\n",
        "\n",
        "  '''\n",
        "  Action format is values 0-7\n",
        "  \n",
        "  0 = (1, 0)\n",
        "  1 = (1, 1)\n",
        "  2 = (0, 1)\n",
        "  3 = (-1,1)\n",
        "  4 = (-1,0)\n",
        "  5 = (-1,-1)\n",
        "  6 = (0,-1)\n",
        "  7 = (1,-1)\n",
        "  '''\n",
        "  def is_valid_(self, agent, action): # need to make sure that the move works, otherwise agent doesn't move and receives penalty\n",
        "    assert 0 <= action <= 7\n",
        "    move = move_dict[action]\n",
        "\n",
        "    return self.grid[agent.x + move[0], agent.y + move[1]] == 0 # anything that isn't passable isn't considered invalid\n",
        "\n",
        "  '''\n",
        "  Ends if we have completed entire grid or we exceed the max number of steps\n",
        "  '''\n",
        "  def env_done_(self):\n",
        "    return np.all(self.grid == 1) or self.step >= self.max_steps\n",
        "\n",
        "  def calc_uncovered_ratio_(self):\n",
        "    return np.sum(self.uncovered_grid) / (self.size * self.size)\n",
        "\n",
        "  '''\n",
        "  joint_action = list of actions to gake for each agent\n",
        "\n",
        "  action cannot move into the space of inpassable terrain / action cannot occupy the space of another agent / cannot go out of bounds\n",
        "  out of bounds is encoded as impassible terrain\n",
        "  '''\n",
        "  def step_(self, joint_actions):\n",
        "    done = False\n",
        "    info = { agent:\n",
        "              {\n",
        "                \"complete\":0, \"group_uncover\":0, \"individual_uncover\":0, \"visitation_penalty\":0, \"collision_penalty\":0\n",
        "              } for agent in self.agents\n",
        "           }\n",
        "\n",
        "    for agent, action in zip(self.agents, joint_actions):\n",
        "      action = action.item()\n",
        "      if not self.is_valid_(agent, action): # Don't move if the action isn't valid\n",
        "        self.rewards[agent] += COLLISION_PENALTY # Reward 5\n",
        "        info[agent][\"collision_penalty\"] = COLLISION_PENALTY\n",
        "      else: # If the movement is valid that take it\n",
        "        self.grid[agent.x, agent.y] = 0 # We assume the cell it occupied was passable so just set it back\n",
        "        agent.move(action)\n",
        "        self.grid[agent.x, agent.y] = 2 # Update the new cell as containing the agent\n",
        "\n",
        "      self.set_observed_cells_(agent) # fill in the cells that the agent has observed\n",
        "      if self.env_done_(): # If the entire grid has been observed then terminate\n",
        "        done = True\n",
        "        self.rewards[agent] += COMPLETION_REWARD # Reward 1\n",
        "        info[agent][\"complete\"] = COMPLETION_REWARD\n",
        "\n",
        "      current_uncovered_ratio = self.calc_uncovered_ratio_()\n",
        "      individual_fraction_uncovered_reward = current_uncovered_ratio - self.last_move_uncovered_ratio\n",
        "      self.last_move_uncovered_ratio = current_uncovered_ratio\n",
        "\n",
        "      if individual_fraction_uncovered_reward <= 1e-5:\n",
        "        self.rewards[agent] += VISITATION_PENALTY # Reward 4\n",
        "        info[agent][\"visitation_penalty\"] = VISITATION_PENALTY\n",
        "      else:\n",
        "        self.rewards[agent] += individual_fraction_uncovered_reward # Reward 3\n",
        "        info[agent][\"individual_uncover\"] = individual_fraction_uncovered_reward\n",
        "\n",
        "    final_uncovered_ratio = self.calc_uncovered_ratio_() # compute final uncovered reward ratio for agent\n",
        "    timestep_fraction_uncovered_reward = final_uncovered_ratio - self.step_start_uncovered_ratio\n",
        "    self.step_start_uncovered_ratio = final_uncovered_ratio\n",
        "\n",
        "    for agent in self.agents: # update each agents reward\n",
        "      self.rewards[agent] += timestep_fraction_uncovered_reward # Reward 2\n",
        "      info[agent][\"group_uncover\"] = timestep_fraction_uncovered_reward\n",
        "\n",
        "    self.step += 1 # go to the next step\n",
        "    obs = self.observe_()\n",
        "    reward = self.rewards_(info)\n",
        "    done = np.array([done] * NUM_AGENTS)\n",
        "    return obs, reward, done\n",
        "\n",
        "  '''\n",
        "  Reward is computed on 5 elements\n",
        "\n",
        "  1 = team-based terminal reward given after completing the grid\n",
        "  2 = team-based progress reward based on the fraction of uncovered cells during timestep\n",
        "  3 = individual discovery reward for cells uncovered\n",
        "  4 = individual visitation penalty if agent didn't uncover any cells\n",
        "  5 = individual collision penalty if agent collided with terrain or went out of bounds\n",
        "  '''\n",
        "  def get_agent_reward_(self, agent):\n",
        "    return self.rewards[agent]\n",
        "\n",
        "  '''\n",
        "  Return, as tuple, all the observations for the agents as defined by their sensor_package\n",
        "  '''\n",
        "  def observe_(self):\n",
        "    obs = []\n",
        "    for agent in self.agents:\n",
        "      obs.append(self.grid[agent.x-agent.sensor_package:agent.x+agent.sensor_package, agent.y-agent.sensor_package:agent.y+agent.sensor_package])\n",
        "    return np.array(obs)\n",
        "\n",
        "  def rewards_(self, reward_dict):\n",
        "    #adding up rewards in the current step\n",
        "    rewards = []\n",
        "    for agent, reward in reward_dict.items():\n",
        "      rewards.append(np.sum(list(reward.values())))\n",
        "    return np.array(rewards).astype(np.float32)\n",
        "\n",
        "\n",
        "  def set_observed_cells_(self, agent):\n",
        "    x,y = agent.x, agent.y\n",
        "    # actual grid is padded but observed cells are not so we unpad\n",
        "    x -= self.padding # readjust to account for padding offset\n",
        "    y -= self.padding\n",
        "    x_min, x_max = x-agent.sensor_package, x+agent.sensor_package\n",
        "    y_min, y_max = y-agent.sensor_package, y+agent.sensor_package\n",
        "\n",
        "    x_min, x_max = np.maximum(0, x_min), np.minimum(self.size, x_max) # make sure that we aren't going passed the boundaries\n",
        "    y_min, y_max = np.maximum(0, y_min), np.minimum(self.size, y_max)\n",
        "    self.uncovered_grid[x_min: x_max, y_min: y_max] = 1\n",
        "\n",
        "  def get_state_(self):\n",
        "    #extracting grid without padding\n",
        "    return self.grid[0+self.padding:self.size+self.padding, 0+self.padding:self.size+self.padding]\n",
        "\n",
        "  def render_(self):\n",
        "    for i in range(self.padding, self.size + self.padding):\n",
        "      for j in range(self.padding, self.size + self.padding):\n",
        "        current = self.grid[i, j]\n",
        "        if current == 0:\n",
        "          print(\" \", end=\"\")#passable terrin\n",
        "        elif current == 1:\n",
        "          print(\"■\", end=\"\")#obstacles\n",
        "        else:\n",
        "          print(\"X\", end=\"\")#agent\n",
        "      print()\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "5YZWVYF5RAqN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up Agent"
      ],
      "metadata": {
        "id": "qJTpYgkmVp1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "  def __init__(self, static_policy=False, env=None):\n",
        "    self.device = DEVICE\n",
        "\n",
        "    self.gamma = GAMMA\n",
        "    self.lr = LR\n",
        "    self.value_loss_weight = VALUE_LOSS_WEIGHT\n",
        "    self.entropy_loss_weight = ENTROPY_LOSS_WEIGHT\n",
        "    self.rollout = ROLLOUT\n",
        "    self.grad_norm_max = GRAD_NORM_MAX\n",
        "\n",
        "    self.static_policy = static_policy\n",
        "   \n",
        "    self.env = env\n",
        "    self.agents = env.agents\n",
        "    self.num_feats = env.observation_space #just number of features\n",
        "    self.num_actions = env.action_space\n",
        "    self.state_space = env.state_space\n",
        "    \n",
        "    self.critic_network = CriticNetwork(self.state_space)\n",
        "    self.optimizer = optim.RMSprop(self.critic_network.parameters(), lr=LR, alpha=0.99, eps=1e-5)\n",
        "    self.declare_networks() #declare networks for num_agents\n",
        "    self.rollouts = self.rollouts = RolloutStorage(self.rollout, NUM_AGENTS,\n",
        "            self.num_feats, self.state_space, self.env.action_space, self.device)\n",
        "\n",
        "    self.value_losses = {}\n",
        "    self.entropy_losses = {}\n",
        "    self.policy_losses = {}\n",
        "    \n",
        "  def declare_networks(self):\n",
        "    for agent in self.agents:\n",
        "      agent.declare_networks()\n",
        "\n",
        "  def get_action(self, obs, state, deterministic=True): # FINISHED?\n",
        "    values, actions, action_log_probs = [], [], []\n",
        "    for agent, current_obs in zip(self.agents, obs):\n",
        "      logits = agent.model(current_obs)\n",
        "      value  = self.critic_network(state)\n",
        "      dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "      if deterministic: #if u want agent to always do the same thing then just take the highest value from distribution\n",
        "          action = dist.probs.argmax(keepdim=True) # removed dim=1\n",
        "      else:\n",
        "          action = dist.sample().view(-1, 1) #other sample from distribution\n",
        "\n",
        "      #action_log_prob = F.log_softmax(logits, dim=0) # removed dim=1, used to be log_probs\n",
        "      #action_log_prob = log_probs.gather(1, actions)\n",
        "\n",
        "      log_prob = F.log_softmax(logits, dim=0) #percent of chance of this action being good under current policy\n",
        "      action_log_prob = log_prob.gather(0, action) #reformat logprob so its in the shape of action, if there are 8 actions there should be 8 probabilities \n",
        "\n",
        "      values.append(value)\n",
        "      actions.append(action)\n",
        "      action_log_probs.append(action_log_prob)\n",
        "    \n",
        "    return torch.stack(values), torch.stack(actions), torch.stack(action_log_probs)\n",
        "\n",
        "  def get_actions_only(self):\n",
        "    actions = []\n",
        "    for agent, o in (self.agents, env.observe_()):\n",
        "      action = agent.model(o)\n",
        "      actions.append(action)\n",
        "    return actions # Decouples the critic from the value function\n",
        "\n",
        "  def evaluate_actions(self, obs, state, actions):\n",
        "    values, action_log_probs, dist_entropys = [], [], []\n",
        "    for agent, current_obs, action in zip(self.agents, obs, actions):\n",
        "      logits = agent.model(current_obs)\n",
        "      value = self.critic_network(state)\n",
        "\n",
        "      dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "      log_prob = F.log_softmax(logits, dim=1)\n",
        "      action_log_prob = log_prob.gather(1, action.view(-1,1))\n",
        "\n",
        "      dist_entropy = dist.entropy().mean() #TODO\n",
        "\n",
        "      values.append(value)\n",
        "      action_log_probs.append(action_log_prob)\n",
        "      dist_entropys.append(dist_entropy)\n",
        "    return values, action_log_probs, dist_entropys \n",
        "\n",
        "  def get_values(self, obs, state): \n",
        "    values = []\n",
        "    for agent, current_obs in zip(self.agents, obs):\n",
        "      value = self.critic_network(state)\n",
        "      values.append(value)\n",
        "\n",
        "    return torch.stack(values)\n",
        "\n",
        "  def compute_loss(self):\n",
        "    obs_shape = self.rollouts.observations.size()[2:]\n",
        "    state_shape = self.rollouts.state.size()[1:]\n",
        "    action_shape = self.rollouts.actions.size()[-1]\n",
        "    num_steps, num_processes, _ = self.rollouts.rewards.size() \n",
        "\n",
        "    losses, action_losses, value_losses, dist_entropys = [],[],[],[]\n",
        "\n",
        "    values, action_log_probs, dist_entropys = self.evaluate_actions(\n",
        "        self.rollouts.observations[:-1].view(NUM_AGENTS, -1, *obs_shape),\n",
        "        self.rollouts.state[:-1], \n",
        "        self.rollouts.actions.view(NUM_AGENTS, -1))\n",
        "    \n",
        "    for agent, value, action_log_prob, dist_entropy, returns in zip(self.agents, values, action_log_probs, dist_entropys, self.rollouts.returns[:-1].view(NUM_AGENTS, -1)): # reshape to num_agents\n",
        "      advantages = returns - value.view(num_steps, 1) \n",
        "      value_loss = advantages.pow(2).mean() \n",
        "\n",
        "      action_loss = -(advantages.detach() * action_log_prob.view(num_steps, 1)).mean()\n",
        "      loss = action_loss + self.value_loss_weight * value_loss - self.entropy_loss_weight * dist_entropy\n",
        "\n",
        "      losses.append(loss), action_losses.append(action_loss), value_losses.append(value_loss), dist_entropys.append(dist_entropy)\n",
        "    state = self.rollouts.state[:-1]\n",
        "    critic_loss = (self.rollouts.returns[:-1].mean(dim=1) - self.critic_network(state)).mean()\n",
        "\n",
        "    return losses, action_losses, value_losses, dist_entropys, critic_loss\n",
        "\n",
        "  def update(self): \n",
        "    losses, action_losses, value_losses, dist_entropys, critic_loss = self.compute_loss()\n",
        "\n",
        "    for agent, loss, action_loss, value_loss, dist_entropy in zip(self.agents, losses, action_losses, value_losses, dist_entropys):\n",
        "      agent.optimizer.zero_grad()\n",
        "      loss.backward(retain_graph=True) \n",
        "      torch.nn.utils.clip_grad_norm_(agent.model.parameters(), self.grad_norm_max)\n",
        "      agent.optimizer.step()\n",
        "      self.save_loss(loss.item(), action_loss.item(), value_loss.item(), dist_entropy.item(), agent) #save updated loss\n",
        "\n",
        "\n",
        "    critic_loss.backward(retain_graph=True)\n",
        "    torch.nn.utils.clip_grad_norm_(self.critic_network.parameters(), self.grad_norm_max)\n",
        "    self.optimizer.step()\n",
        "\n",
        "    return value_losses, action_losses, dist_entropys, critic_loss\n",
        "\n",
        "  def save_loss(self, loss, policy_loss, value_loss, entropy_loss, agent):\n",
        "    if agent not in self.policy_losses:\n",
        "      self.policy_losses[agent] = [policy_loss]\n",
        "    else:\n",
        "      self.policy_losses[agent].append(policy_loss)\n",
        "\n",
        "    if agent not in self.value_losses:\n",
        "      self.value_losses[agent] = [value_loss]\n",
        "    else:\n",
        "      self.value_losses[agent] = [value_loss]\n",
        "\n",
        "    if agent not in self.entropy_losses:\n",
        "      self.entropy_losses[agent] = [entropy_loss]\n",
        "    else:\n",
        "      self.entropy_losses[agent].append(entropy_loss)"
      ],
      "metadata": {
        "id": "hjUuxHCTFii1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "BjhG0ZmLrmTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print(episode_idx, total_step_num, dist_entropy, value_loss, action_loss, critic_loss, steps):\n",
        "  print(f\"{episode_idx} | {episode_idx} / {ENV_STEPS+1}\")\n",
        "  print(\"-------------------------------------------------\")\n",
        "  idx = 0\n",
        "  for e, v, a in zip(dist_entropy, value_loss, action_loss):\n",
        "    print(f\"Agent {idx}: Dist Entropy: {e} | Value Loss: {v} | Action Loss: {a}\")\n",
        "    idx += 1\n",
        "  print(\"Steps Required to Solve:\", steps)\n",
        "  print(\"Critic Loss:\", critic_loss)"
      ],
      "metadata": {
        "id": "jSkdJ64EScjx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "\n",
        "    env = UnknownTerrain()\n",
        "\n",
        "    obs_shape = env.observation_space\n",
        "    state_shape = env.state_space\n",
        "    model = Model(static_policy=False, env=env)\n",
        "\n",
        "    current_obs = torch.zeros(NUM_AGENTS, *obs_shape,\n",
        "                    device=DEVICE, dtype=torch.float)\n",
        "    current_state = torch.zeros(*state_shape,\n",
        "                    device=DEVICE, dtype=torch.float)\n",
        "    #create arbitary obs/state to start the training process, load data into those tensors once we have data\n",
        "    def update_current_obs(obs):\n",
        "      return torch.from_numpy(obs.astype(np.float32)).to(DEVICE)\n",
        "\n",
        "    def update_current_state(state):\n",
        "      return torch.from_numpy(state.astype(np.float32)).to(DEVICE)\n",
        "\n",
        "    obs = env.reset() \n",
        "    current_obs = update_current_obs(obs)\n",
        "    state = env.get_state_()\n",
        "    current_state = update_current_state(state)\n",
        "\n",
        "    model.rollouts.observations[0].copy_(current_obs)\n",
        "    model.rollouts.state[0].copy_(current_state)#add into rollout storage\n",
        "    \n",
        "    episode_rewards = np.zeros(NUM_AGENTS, dtype=np.float32)\n",
        "    final_rewards = np.zeros(NUM_AGENTS, dtype=np.float32)\n",
        "\n",
        "    start=timer()\n",
        "\n",
        "    print_step = 1\n",
        "    print_threshold = 10\n",
        "    best_steps_required = 100\n",
        "    current_steps_required = 0\n",
        "    \n",
        "    for episode_idx in range(1, ENV_STEPS+1):\n",
        "        for step in range(ROLLOUT):\n",
        "            with torch.no_grad():\n",
        "                values, actions, action_log_prob = model.get_action(model.rollouts.observations[step], model.rollouts.state[step]) \n",
        "            cpu_actions = actions.view(-1).cpu().numpy()#take it fr gpu put it into cpu convert to numpy, desont do anything if already on cpu\n",
        "\n",
        "            obs, reward, done = env.step_(cpu_actions)# step env according to actions\n",
        "            episode_rewards = np.add(reward, episode_rewards)#put rewards together\n",
        "            masks = 1. - done.astype(np.float32)#take out rewards doesnt matter, mask is boolean\n",
        "            final_rewards *= masks\n",
        "            final_rewards = np.add(reward, (1. - masks) * episode_rewards)\n",
        "            episode_rewards *= masks\n",
        "\n",
        "\n",
        "            rewards = torch.from_numpy(reward.astype(np.float32)).view(-1, 1).to(DEVICE)\n",
        "            masks = torch.from_numpy(masks).to(DEVICE).view(-1, 1)\n",
        "            current_obs *= masks.view(-1, 1, 1)#remove observations fr terminal states\n",
        "            current_obs = update_current_obs(obs)\n",
        "            current_state = torch.from_numpy(env.get_state_().astype(np.float32)).to(DEVICE)\n",
        "\n",
        "            if(np.all(done)):\n",
        "              current_steps_required = env.step\n",
        "              env.reset()\n",
        "              #if in terminal state, reset and statrt again\n",
        "\n",
        "              if current_steps_required < best_steps_required: #keep track of how fast agents solve it \n",
        "                best_steps_required = current_steps_required\n",
        "\n",
        "\n",
        "            model.rollouts.insert(current_obs, actions.view(-1, 1), action_log_prob, values, rewards, masks, current_state)\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            next_value = model.get_values(model.rollouts.observations[-1], model.rollouts.state[-1]) #compute next value\n",
        "\n",
        "        model.rollouts.compute_returns(next_value, GAMMA)#compute returns from next value\n",
        "        value_loss, action_loss, dist_entropy, critic_loss = model.update()\n",
        "\n",
        "        model.rollouts.after_update()\n",
        "\n",
        "        if episode_idx % 100 == 0:\n",
        "            try:\n",
        "                # clear_output()\n",
        "                end = timer()\n",
        "                total_num_steps = (episode_idx + 1) * NUM_AGENTS * ROLLOUT\n",
        "                '''\n",
        "                print(\"Updates {}, Num Timesteps {}, FPS {},\\nMean/Median Reward {:.1f}/{:.1f}, Min/Max Reward {:.1f}/{:.1f},\\nEntropy {:.5f}, Value Loss {:.5f}, Policy Loss {:.5f}\".\n",
        "                format(episode_idx, total_num_steps,\n",
        "                       int(total_num_steps / (end - start)),\n",
        "                       np.mean(final_rewards),\n",
        "                       np.median(final_rewards),\n",
        "                       np.min(final_rewards),\n",
        "                       np.max(final_rewards), dist_entropy,\n",
        "                       value_loss, action_loss))\n",
        "                '''\n",
        "                pretty_print(episode_idx, total_num_steps, dist_entropy, value_loss, action_loss, critic_loss, best_steps_required)\n",
        "            except IOError:\n",
        "                pass"
      ],
      "metadata": {
        "id": "mB2cwqL00zW5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b20c4f5-f766-439f-e3eb-40475aebd754"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 | 100 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 7.8451672949020335e-28 | Value Loss: 841959.875 | Action Loss: -284654.1875\n",
            "Agent 1: Dist Entropy: 1.0884332656860352 | Value Loss: 324731.125 | Action Loss: -17992.58984375\n",
            "Agent 2: Dist Entropy: 1.9007153511047363 | Value Loss: 30713.1640625 | Action Loss: -117.13890075683594\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-506.3564, grad_fn=<MeanBackward0>)\n",
            "200 | 200 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 281978.21875 | Action Loss: -779920.25\n",
            "Agent 1: Dist Entropy: 2.0455002969210767e-10 | Value Loss: 39542.34765625 | Action Loss: -9994.6533203125\n",
            "Agent 2: Dist Entropy: 1.3302711248397827 | Value Loss: 149684.703125 | Action Loss: 306.63232421875\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-108.7857, grad_fn=<MeanBackward0>)\n",
            "300 | 300 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 295003.5625 | Action Loss: -1823763.25\n",
            "Agent 1: Dist Entropy: 1.9478048654114957e-43 | Value Loss: 43176.41796875 | Action Loss: -70855.578125\n",
            "Agent 2: Dist Entropy: 0.03189611807465553 | Value Loss: 141542.828125 | Action Loss: 13.555059432983398\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-119.6449, grad_fn=<MeanBackward0>)\n",
            "400 | 400 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 277257.125 | Action Loss: -2538954.25\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 36536.96484375 | Action Loss: -98547.9921875\n",
            "Agent 2: Dist Entropy: 0.09601637721061707 | Value Loss: 153513.234375 | Action Loss: 61.2204704284668\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-99.6447, grad_fn=<MeanBackward0>)\n",
            "500 | 500 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 275742.125 | Action Loss: -2716810.0\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 35408.328125 | Action Loss: -179877.5625\n",
            "Agent 2: Dist Entropy: 0.02493521384894848 | Value Loss: 151022.421875 | Action Loss: 15.695998191833496\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-97.3123, grad_fn=<MeanBackward0>)\n",
            "600 | 600 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 295589.3125 | Action Loss: -4424936.5\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 40287.83203125 | Action Loss: -411666.8125\n",
            "Agent 2: Dist Entropy: 0.18275628983974457 | Value Loss: 135563.984375 | Action Loss: 37.294456481933594\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-115.3000, grad_fn=<MeanBackward0>)\n",
            "700 | 700 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 198314.5 | Action Loss: -5550987.5\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 40492.65234375 | Action Loss: -74406.7421875\n",
            "Agent 2: Dist Entropy: 0.05347917228937149 | Value Loss: 153613.21875 | Action Loss: 5.008615493774414\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-5.1739, grad_fn=<MeanBackward0>)\n",
            "800 | 800 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 301535.3125 | Action Loss: -8301660.5\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 41014.48046875 | Action Loss: -1032120.9375\n",
            "Agent 2: Dist Entropy: 0.016740063205361366 | Value Loss: 121334.9765625 | Action Loss: 2.4863812923431396\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-122.4501, grad_fn=<MeanBackward0>)\n",
            "900 | 900 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 330782.8125 | Action Loss: -10266879.0\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 47378.36328125 | Action Loss: -782450.9375\n",
            "Agent 2: Dist Entropy: 0.03755786642432213 | Value Loss: 109006.5390625 | Action Loss: 153.5618438720703\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-143.1653, grad_fn=<MeanBackward0>)\n",
            "1000 | 1000 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 319980.03125 | Action Loss: -30214366.0\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 43827.140625 | Action Loss: -1199930.375\n",
            "Agent 2: Dist Entropy: 0.03354092687368393 | Value Loss: 103791.7734375 | Action Loss: 6.362177848815918\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-137.8613, grad_fn=<MeanBackward0>)\n",
            "1100 | 1100 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 208512.15625 | Action Loss: -9471838.0\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 38016.3203125 | Action Loss: -308848.15625\n",
            "Agent 2: Dist Entropy: 0.057574424892663956 | Value Loss: 117478.234375 | Action Loss: 7.201214790344238\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-28.9139, grad_fn=<MeanBackward0>)\n",
            "1200 | 1200 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 246002.125 | Action Loss: -24484106.0\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 25308.716796875 | Action Loss: -1916546.25\n",
            "Agent 2: Dist Entropy: 1.4435367120313458e-05 | Value Loss: 135431.25 | Action Loss: 0.0003791532653849572\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-70.1591, grad_fn=<MeanBackward0>)\n",
            "1300 | 1300 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 264619.15625 | Action Loss: -82111704.0\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 29007.3828125 | Action Loss: -1251673.75\n",
            "Agent 2: Dist Entropy: 0.01103381160646677 | Value Loss: 120966.0 | Action Loss: 12.738351821899414\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-87.4735, grad_fn=<MeanBackward0>)\n",
            "1400 | 1400 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 258625.890625 | Action Loss: -72415736.0\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 26735.974609375 | Action Loss: -815677.875\n",
            "Agent 2: Dist Entropy: 0.0737815648317337 | Value Loss: 113457.9609375 | Action Loss: 38.5796012878418\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-86.4302, grad_fn=<MeanBackward0>)\n",
            "1500 | 1500 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 250950.703125 | Action Loss: -76291080.0\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 25305.296875 | Action Loss: -731707.3125\n",
            "Agent 2: Dist Entropy: 0.014373458921909332 | Value Loss: 112420.53125 | Action Loss: 28.926631927490234\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-80.1592, grad_fn=<MeanBackward0>)\n",
            "1600 | 1600 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 263788.5625 | Action Loss: -185697728.0\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 26972.345703125 | Action Loss: -877198.75\n",
            "Agent 2: Dist Entropy: 0.007864082232117653 | Value Loss: 96738.5703125 | Action Loss: 0.8840090036392212\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-96.3945, grad_fn=<MeanBackward0>)\n",
            "1700 | 1700 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 249420.90625 | Action Loss: -112477264.0\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 24930.9296875 | Action Loss: -2789427.75\n",
            "Agent 2: Dist Entropy: 0.0001556281204102561 | Value Loss: 98312.328125 | Action Loss: 0.003933175001293421\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-84.8808, grad_fn=<MeanBackward0>)\n",
            "1800 | 1800 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 163746.515625 | Action Loss: -105149824.0\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 34651.69140625 | Action Loss: 2864802.75\n",
            "Agent 2: Dist Entropy: 0.08250918239355087 | Value Loss: 104843.8046875 | Action Loss: 17.5880126953125\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(6.9241, grad_fn=<MeanBackward0>)\n",
            "1900 | 1900 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 162056.015625 | Action Loss: -109475528.0\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 35093.24609375 | Action Loss: 3458597.25\n",
            "Agent 2: Dist Entropy: 0.009629841893911362 | Value Loss: 97790.6953125 | Action Loss: 3.5569100379943848\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(3.7497, grad_fn=<MeanBackward0>)\n",
            "2000 | 2000 / 10001\n",
            "-------------------------------------------------\n",
            "Agent 0: Dist Entropy: 0.0 | Value Loss: 211826.15625 | Action Loss: -168397328.0\n",
            "Agent 1: Dist Entropy: 0.0 | Value Loss: 22641.966796875 | Action Loss: -340890.5625\n",
            "Agent 2: Dist Entropy: 0.0316183902323246 | Value Loss: 107900.0078125 | Action Loss: 67.62439727783203\n",
            "Steps Required to Solve: 100\n",
            "Critic Loss: tensor(-47.5625, grad_fn=<MeanBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4346a97490ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#compute returns from next value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-c8374cc23027>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_norm_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m       \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#save updated loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/rmsprop.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             rmsprop(params_with_grad,\n\u001b[0m\u001b[1;32m    151\u001b[0m                     \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0msquare_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/rmsprop.py\u001b[0m in \u001b[0;36mrmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, foreach, maximize, differentiable, lr, alpha, eps, weight_decay, momentum, centered)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_rmsprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    202\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m          \u001b[0msquare_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/rmsprop.py\u001b[0m in \u001b[0;36m_single_tensor_rmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, lr, alpha, eps, weight_decay, momentum, centered, maximize, differentiable)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training isn't occuring properly because the embedding layer isn't being updated according to triplet loss."
      ],
      "metadata": {
        "id": "YgId4QZ65Ah1"
      }
    }
  ]
}